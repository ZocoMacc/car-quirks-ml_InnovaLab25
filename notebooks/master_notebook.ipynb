{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & Settings\n",
        "- Descargar e importar dependencias"
      ],
      "metadata": {
        "id": "ryKe98J-Bu0M"
      },
      "id": "ryKe98J-Bu0M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dummy baseline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_squared_error\n",
        "\n",
        "# Preprocessing (Pipelines)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Modeling: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Modeling: Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Modeling: LightGBM\n",
        "# Suprimir warnings (LGBM suele ser sucio con las warnings)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Unknown parameter.*\")\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import warnings"
      ],
      "metadata": {
        "id": "eE1_V_wWBq4H"
      },
      "id": "eE1_V_wWBq4H",
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "- Cargar el data set\n",
        "- Primer vistazo a la distribucion"
      ],
      "metadata": {
        "id": "QctRhgn6B40Z"
      },
      "id": "QctRhgn6B40Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get url from file hosted on GitHub\n",
        "url = \"https://raw.githubusercontent.com/ZocoMacc/car-quirks-ml_InnovaLab25/refs/heads/main/data/cars_data.csv\"\n",
        "\n",
        "# Cargar csv en un DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Explorar la estructura del DataFrame (Sanity Check)\n",
        "df.info()\n",
        "df.shape\n",
        "# df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzqNHCXUCZTN",
        "outputId": "0757e409-29c0-4f57-b6c9-43b83f57a295"
      },
      "id": "EzqNHCXUCZTN",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   name                    10000 non-null  object \n",
            " 1   year                    10000 non-null  int64  \n",
            " 2   selling_price           10000 non-null  int64  \n",
            " 3   km_driven               10000 non-null  int64  \n",
            " 4   fuel                    10000 non-null  int64  \n",
            " 5   combustible_estimado_l  10000 non-null  float64\n",
            " 6   seller_type             10000 non-null  int64  \n",
            " 7   transmission            10000 non-null  int64  \n",
            " 8   owner                   10000 non-null  int64  \n",
            " 9   tipo_carroceria         10000 non-null  int64  \n",
            " 10  potencia_motor_hp       10000 non-null  int64  \n",
            " 11  nivel_seguridad         10000 non-null  float64\n",
            " 12  calidad_auto            10000 non-null  object \n",
            " 13  score_calidad           10000 non-null  float64\n",
            " 14  eficiencia_km_l         10000 non-null  float64\n",
            "dtypes: float64(4), int64(9), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el balance de las clases (84% Media, 10.5% Alta, 5.5% Baja)\n",
        "df[\"calidad_auto\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "tfZjya5Ya2MI",
        "outputId": "1344e8c2-e039-4138-f865-ab2a319f5fde"
      },
      "id": "tfZjya5Ya2MI",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "calidad_auto\n",
              "Media    0.8399\n",
              "Alta     0.1052\n",
              "Baja     0.0549\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>calidad_auto</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Media</th>\n",
              "      <td>0.8399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Alta</th>\n",
              "      <td>0.1052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Baja</th>\n",
              "      <td>0.0549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- Un fuerte desbalanceo de clases\n",
        "  - \"Media\" es la mayoria con un 84%\n",
        "  - \"Alta\" solo cubre el 10%, y \"Baja\" solo un 5.5%\n",
        "- El accuracy de referencia (baseline) es engañosamente alto\n",
        "  - Un DummyClassifier que siempre predice \"Media\" deberia de poder predecir 83.99%.\n",
        "  - Superar este baseline no garantiza que el modelo sea mas precizo detectando \"Alta\" o \"Baja\".\n",
        "- Riesgo de abandono de las clases minoritarias\n",
        "  - El modelo tiene el riesgo de predecir \"Media\" la mayoria del tiempo debido al desbalanceo de las clases, lo que podria ignorar \"Alta\" o \"Baja\" como opciones."
      ],
      "metadata": {
        "id": "4s-HoOX8IJAh"
      },
      "id": "4s-HoOX8IJAh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning & Dropping\n",
        "- Decidir las variables que seran ignoradas por el modelo.\n",
        "- Si es necesarion, transformar valores en interpretaciones legibles por el modelo o marcarlas con una categoria."
      ],
      "metadata": {
        "id": "EujHrAybfup-"
      },
      "id": "EujHrAybfup-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Duda sobre la variable \"score_calidad\": Que tanto se correlaciona con \"calidad_auto\"\n",
        "# Mapear las categorias con numeros para calcular la correlacion\n",
        "mapping = {\"Baja\": 0, \"Media\": 1, \"Alta\": 2}\n",
        "df[\"calidad_code\"] = df[\"calidad_auto\"].map(mapping)\n",
        "\n",
        "# Calcular la correlacion\n",
        "corr = df[[\"score_calidad\", \"calidad_code\"]].corr().iloc[0, 1]\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Correlacion entre score_calidad y calidad_auto: {corr:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA-E2kkjgLhv",
        "outputId": "e45041d8-bd05-4943-b25a-3112d92f8b60"
      },
      "id": "kA-E2kkjgLhv",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlacion entre score_calidad y calidad_auto: 0.740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- La correlacion entre \"score_calidad\" y \"calidad_auto\" no es 1.0, lo que quiere decir que \"calidad_auto\" no depende al 100% de la variable \"score_calidad\".\n",
        "- La variable \"score_calidad\" va a ser considerada."
      ],
      "metadata": {
        "id": "ZTNtSbS-hsXo"
      },
      "id": "ZTNtSbS-hsXo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extraer marca del nombre\n",
        "- Usar el nombre y cortarlo de una manera en la que podamos extrar la marca del auto para poder usarlo como variable y ser considerada."
      ],
      "metadata": {
        "id": "W7UU6SNHlHoE"
      },
      "id": "W7UU6SNHlHoE"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Rifense este rollo porfi\n",
        "- Extraer el nombre de la marca de cada carro (crar una columna para \"brand\")\n",
        "- Investigar si es viable extraer tambien el submodelo del carro (Limited, Sport, AC, ZX, etc)\n",
        "  - Maybe no nos conviene por que son un chingo pero maybe si si no son tantas quien sabe\n",
        "  - Si si es viable crear una columna para \"submodel\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-0SgRIRHmF5z",
        "outputId": "67941b3a-ac7f-4b0f-e5b5-8c085fad1074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "-0SgRIRHmF5z",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRifense este rollo porfi\\n- Extraer el nombre de la marca de cada carro (crar una columna para \"brand\")\\n- Investigar si es viable extraer tambien el submodelo del carro (Limited, Sport, AC, ZX, etc)\\n  - Maybe no nos conviene por que son un chingo pero maybe si si no son tantas quien sabe\\n  - Si si es viable crear una columna para \"submodel\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convertir columnas de enteros en categorias\n",
        "- Columnas como \"fuel\", \"seller_type\", \"transmission\", \"owner\", \"tipo_carroceria\" tienen asignado un valor entero pero en realidad representan categorias sin orden.\n",
        "- Convertir estas variables a Pandas' category dtype para mas claridad y eficiencia en la manipulacion de estas variables, tambien es mas seguro hacerlo de esta forma, asi evitamos que estas columnas se interpreten como numeric imputers o scalers."
      ],
      "metadata": {
        "id": "43A-JGJKnxf2"
      },
      "id": "43A-JGJKnxf2"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_int_cols = [\n",
        "    \"fuel\",\n",
        "    \"seller_type\",\n",
        "    \"transmission\",\n",
        "    \"owner\",\n",
        "    \"tipo_carroceria\"\n",
        "    #\"brand\"             # caegoria extraida del nombre\n",
        "]\n",
        "\n",
        "for col in cat_int_cols:\n",
        "    df[col] = df[col].astype(\"category\")"
      ],
      "metadata": {
        "id": "CZlrE_CHn9ZP"
      },
      "id": "CZlrE_CHn9ZP",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Droppear columnas innecesarias\n",
        "- Calcular correlaciones entre las columnas para identificar cual tiene un gran efecto en \"calidad_auto\"\n",
        "- Definir X, y para entrenamiento"
      ],
      "metadata": {
        "id": "xJL2hxc2NKNk"
      },
      "id": "xJL2hxc2NKNk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Al final se droppea el nombre (ya habiendo extraido la brand)\n",
        "df = df.drop(columns=[\"name\"])\n",
        "\n",
        "# Ver correlaciones para identificar leaks\n",
        "correlations = df.drop(columns=[\"calidad_auto\"]).corrwith(df[\"calidad_code\"])\n",
        "print(\"Correlaciones\")\n",
        "print(correlations.sort_values(ascending=False))\n",
        "\n",
        "# Tirar la columna helper que definimos previemente\n",
        "df = df.drop(columns=[\"calidad_code\"])\n",
        "\n",
        "print(\"\\nColumnas que permanecen en el DataFrame:\")\n",
        "print(df.dtypes)  # Checar las columnas que permanecen\n",
        "\n",
        "# Definir X & y para el training\n",
        "features_to_drop = [\"calidad_auto\", \"score_calidad\"]\n",
        "X = df.drop(columns=features_to_drop)\n",
        "y = df[\"calidad_auto\"]    # Estableciendo el target\n",
        "\n",
        "# Inspeccionar las variables que permanecen\n",
        "# print(\"Columnas a considerar:\\n\", X.columns.tolist())\n",
        "print(\"\\nFeature dtypes (para X):\")\n",
        "print(X.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te-8G4rdNOT0",
        "outputId": "e5570028-03ed-4c27-c9a3-71f7e42a1259"
      },
      "id": "te-8G4rdNOT0",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlaciones\n",
            "calidad_code              1.000000\n",
            "score_calidad             0.740022\n",
            "eficiencia_km_l           0.526133\n",
            "nivel_seguridad           0.497154\n",
            "year                      0.495617\n",
            "potencia_motor_hp         0.294303\n",
            "transmission              0.197374\n",
            "selling_price             0.103824\n",
            "fuel                      0.031103\n",
            "seller_type               0.028950\n",
            "km_driven                -0.024826\n",
            "tipo_carroceria          -0.033271\n",
            "owner                    -0.045147\n",
            "combustible_estimado_l   -0.107143\n",
            "dtype: float64\n",
            "\n",
            "Columnas que permanecen en el DataFrame:\n",
            "year                         int64\n",
            "selling_price                int64\n",
            "km_driven                    int64\n",
            "fuel                      category\n",
            "combustible_estimado_l     float64\n",
            "seller_type               category\n",
            "transmission              category\n",
            "owner                     category\n",
            "tipo_carroceria           category\n",
            "potencia_motor_hp            int64\n",
            "nivel_seguridad            float64\n",
            "calidad_auto                object\n",
            "score_calidad              float64\n",
            "eficiencia_km_l            float64\n",
            "dtype: object\n",
            "\n",
            "Feature dtypes (para X):\n",
            "year                         int64\n",
            "selling_price                int64\n",
            "km_driven                    int64\n",
            "fuel                      category\n",
            "combustible_estimado_l     float64\n",
            "seller_type               category\n",
            "transmission              category\n",
            "owner                     category\n",
            "tipo_carroceria           category\n",
            "potencia_motor_hp            int64\n",
            "nivel_seguridad            float64\n",
            "eficiencia_km_l            float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DummyClassifier & Baseline\n",
        "- El primer paso es establecer una baseline y asegurarse de que la informacion esta limpia\n",
        "- El objetivo es confirmar ques este DummyClassifier de como resultado una precision de 83.99%\n",
        "- Se usara la estrategia de \"most_frequent\", simplemente el valor que mas se repite sera elegido para la prediccion.\n",
        "- Este modelo representa el piso (baseline) de qualquier modelo futuro.\n",
        "- El split para entrenamiento es un 80/20 asegurandose de que cada clase aparece con las misma proporciones.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ng1LdD5yB6_t"
      },
      "id": "Ng1LdD5yB6_t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Test\n",
        "# print(\"Train class proportions:\\n\", y_train.value_counts(normalize=True))\n",
        "# print(\"\\nTest class proportions:\\n\", y_test.value_counts(normalize=True))\n",
        "\n",
        "# Iniciar DummyClassifier\n",
        "dummy = DummyClassifier(\n",
        "    strategy=\"most_frequent\", # Siempre predice la clase mas frecuente (\"Media\")\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dummy.fit(X_train, y_train)     # \"Entrena\" basado en la clase mas frecuente\n",
        "\n",
        "# Obtener predicciones\n",
        "y_pred = dummy.predict(X_test)  # Aplica la regla a cada fila de X_test\n",
        "\n",
        "# Evaluar accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)   # Fraccion de predicciones correctas\n",
        "print(f\"DummyClassifier Accuracy: {accuracy}\")\n",
        "\n",
        "# Visualizar resulatdos de predicciones\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    digits=4,         # Numero de decimales\n",
        "    zero_division=0   # Ignorar zero-division errors (debido a strings)\n",
        "))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC8kJNPZM2Xt",
        "outputId": "69a11ceb-6e2e-4ac1-ed0f-3d11efa55d34"
      },
      "id": "WC8kJNPZM2Xt",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier Accuracy: 0.84\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.0000    0.0000    0.0000       210\n",
            "        Baja     0.0000    0.0000    0.0000       110\n",
            "       Media     0.8400    1.0000    0.9130      1680\n",
            "\n",
            "    accuracy                         0.8400      2000\n",
            "   macro avg     0.2800    0.3333    0.3043      2000\n",
            "weighted avg     0.7056    0.8400    0.7670      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1680    0    0]\n",
            " [ 210    0    0]\n",
            " [ 110    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lo que esto nos dice\n",
        "- Tenemos un baseline de ~84% accuracy, este es el objetivo a vencer en los proximos modelos."
      ],
      "metadata": {
        "id": "wgRyZpmLYjPP"
      },
      "id": "wgRyZpmLYjPP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "- Preparar el data set para ser procesado\n",
        "- Dividir entre variables numericas y categoricas\n",
        "- Implementar un pipeline para cada tipo de variable\n",
        "- Usar ColumnTransformer para aplicar los pipelines al DataFrame"
      ],
      "metadata": {
        "id": "PSOmYyV3CAqt"
      },
      "id": "PSOmYyV3CAqt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identificar columnas en base al dtype"
      ],
      "metadata": {
        "id": "zx671--0xzfX"
      },
      "id": "zx671--0xzfX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos numericos\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "print(\"Numeric features:\", num_cols)\n",
        "\n",
        "# Datos categoricos\n",
        "cat_cols = X.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "print(\"\\nCategorical features:\", cat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU_JcFDaxLl4",
        "outputId": "c3ab8a86-5d14-4b98-ec93-34ec7d737211"
      },
      "id": "kU_JcFDaxLl4",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features: ['year', 'selling_price', 'km_driven', 'combustible_estimado_l', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n",
            "\n",
            "Categorical features: ['fuel', 'seller_type', 'transmission', 'owner', 'tipo_carroceria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Construir el pipeline numerico"
      ],
      "metadata": {
        "id": "4h0v9hENyWIX"
      },
      "id": "4h0v9hENyWIX"
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipe = Pipeline([\n",
        "    # Definiendo tecnicas de defensive programming\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")), # Imputacion de media\n",
        "    (\"scaler\", StandardScaler())                   # Standard scaling\n",
        "])"
      ],
      "metadata": {
        "id": "vepTzJLPyYXG"
      },
      "id": "vepTzJLPyYXG",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El objetivo de la funcion `SimpleImputer()` es remplazar valores faltantes por el valor definido por su argumento (e.g. median).\n",
        "- El objetivo de la funcion `StandardScaler()` es que despues de una imputacion, el valor numerico es escalado para tener una media = 0 y standard deviation = 1. De esta forma se evitan posibles errores generados por tener diferentes escalas de numeros en las diferentes columnas."
      ],
      "metadata": {
        "id": "cyk6t5x1y9ad"
      },
      "id": "cyk6t5x1y9ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Construir el pipeline categorico"
      ],
      "metadata": {
        "id": "A-TcmKaA0xQd"
      },
      "id": "A-TcmKaA0xQd"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipe = Pipeline([\n",
        "    # Definiendo tecnicas de defensive programming\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Imputacion de moda\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))    # One-hot encoding\n",
        "])"
      ],
      "metadata": {
        "id": "5WypSVy705LP"
      },
      "id": "5WypSVy705LP",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `SimpleImputer()` remplaza cualquier valor faltante por el valor mas frecuente (la moda). Esto preserva la distribucion del data set.\n",
        "- El objetivo del OneHotEncoder es convertir cada columna categorica en las columnas binarias necesarias para representar cada valor categorico unico.\n",
        "  - `handle_unkwnown=\"ignore\"` es util para que en caso de que aparezca una nueva categoria \"vacia\", que esta sea representada por una columna entera de puros ceros."
      ],
      "metadata": {
        "id": "3EmHAM6wBJWM"
      },
      "id": "3EmHAM6wBJWM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementacion de ColumnTransformer para aplicar los pipelines"
      ],
      "metadata": {
        "id": "KlPpE_hsE6D_"
      },
      "id": "KlPpE_hsE6D_"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Aplicar num_pipe a las columnas numericas\n",
        "        (\"nums\", num_pipe, num_cols),\n",
        "        # Aplicar cat_pipe a columnas categoricas\n",
        "        (\"cats\", cat_pipe, cat_cols)\n",
        "    ],\n",
        "    remainder=\"drop\" # Tirar cualquier columna no listada\n",
        ")"
      ],
      "metadata": {
        "id": "a1xq23TBFJSe"
      },
      "id": "a1xq23TBFJSe",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `transformers` es un array de tuples `(name, pipeline, columns)` que ira siendo populado mientras `.fit()` corre\n",
        "  1. `(\"nums\", num_pipe, num_cols)` toma todas las columnas en `num_cols` y aplica el pipeline `num_pipe` y entraga como output el array numerico transformado.\n",
        "  2. `(\"cats\", cat_pipe, cat_cols)` toma todas las columnas en `cat_cols` y aplica el pipeline `cat_pipe` y entraga como output el array categorico transformado."
      ],
      "metadata": {
        "id": "kl43BKMVJVR3"
      },
      "id": "kl43BKMVJVR3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspeccionar el output transformado"
      ],
      "metadata": {
        "id": "q3596Gl4KnfD"
      },
      "id": "q3596Gl4KnfD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Testear el preprocessor\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Ver la forma del array final\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "print(\"Transformed shape:\", X_train_transformed.shape)\n",
        "#print(len(num_cols))\n",
        "\n",
        "# Imprimir las columnas finales\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(\"All output features:\\n\", feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7F5TjTMKufV",
        "outputId": "c3c1ad0e-a800-4411-a632-ff3a04f81cf3"
      },
      "id": "z7F5TjTMKufV",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed shape: (8000, 27)\n",
            "All output features:\n",
            " ['nums__year' 'nums__selling_price' 'nums__km_driven'\n",
            " 'nums__combustible_estimado_l' 'nums__potencia_motor_hp'\n",
            " 'nums__nivel_seguridad' 'nums__eficiencia_km_l' 'cats__fuel_0'\n",
            " 'cats__fuel_1' 'cats__fuel_2' 'cats__fuel_3' 'cats__fuel_4'\n",
            " 'cats__seller_type_0' 'cats__seller_type_1' 'cats__seller_type_2'\n",
            " 'cats__transmission_0' 'cats__transmission_1' 'cats__owner_0'\n",
            " 'cats__owner_1' 'cats__owner_2' 'cats__owner_3' 'cats__owner_4'\n",
            " 'cats__tipo_carroceria_1' 'cats__tipo_carroceria_2'\n",
            " 'cats__tipo_carroceria_3' 'cats__tipo_carroceria_4'\n",
            " 'cats__tipo_carroceria_5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- En total hay 28 columnas ya con los pipelines aplicados (8 numericas y 20 categoricas) - temporal (falta agregar \"brand\" y talvez \"submodel\")\n",
        "- Los prefijos `nums_` y `cats_` nos dicen de cual pipeline proviene cada columna\n",
        "- Estos nombres nos serviran despues para implementacion de funciones o debugging"
      ],
      "metadata": {
        "id": "SZumUwvGMuMR"
      },
      "id": "SZumUwvGMuMR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "- La regresión logística es una técnica de análisis de datos que utiliza las matemáticas para encontrar las relaciones entre dos factores de datos. Luego, utiliza esta relación para predecir el valor de uno de esos factores basándose en el otro. Normalmente, la predicción tiene un número finito de resultados, como un sí o un no."
      ],
      "metadata": {
        "id": "H2t9M4f-R5j2"
      },
      "id": "H2t9M4f-R5j2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un objecto de LogisticRegression\n",
        "logreg = LogisticRegression(\n",
        "    multi_class=\"multinomial\",   # Clasificacion multiclase\n",
        "    solver=\"saga\",               # Algoritmo de optimizacion (soporta L1/L2)\n",
        "    class_weight=\"balanced\",     # Ajuste de pesos para clases desbalanceadas\n",
        "    max_iter=1000,               # Numero maximo de iteraciones\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Construir un unico pipeline\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", logreg)\n",
        "])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el modelo de regresion logistica\n",
        "cv_macro_f1 = cross_val_score(\n",
        "    lr_pipeline,    # Pipeline completeo\n",
        "    X,              # Todas las features\n",
        "    y,              # Todas las labels\n",
        "    cv=5,           # 5 folds\n",
        "    scoring=\"f1_macro\", # macro-averaged F1\n",
        "    n_jobs=-1       # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "# Imprimir los fold scores y la media\n",
        "print(\"Logistic Regression 5-fold CV macro-F1 scores:\", cv_macro_f1.round(4))\n",
        "print(\"Mean macro-F1:\", cv_macro_f1.mean().round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSvhbZ08R-Ig",
        "outputId": "faaf1ab8-76aa-4d80-c889-6c8e5be06f9e"
      },
      "id": "wSvhbZ08R-Ig",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression 5-fold CV macro-F1 scores: [0.947  0.919  0.91   0.8839 0.8849]\n",
            "Mean macro-F1: 0.909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entendiendo la media Macro-F1\n",
        "- La evaluacion multiclase Macro-F1 toma el promedio no ponderado de los tres puntajes F1 por clase (\"Alta\", \"Media\", \"Baja\"). Este proceso toma cada clase de forma equivalente aunque la distribucion sea diferente (10.5%, 84%, 5.5%). Usando macro-F1 nos aseguramos de que el modelo no esta eligiendo \"Media\" en todas las predicciones y realmente esta prediciendo entre las tres clases.\n",
        "- El metodo de evaluacion 5-fold cross-validation es una manera de estimar que tan bien el pipeline (preprocessor + modelo) se generaliza a datasets no vistos\n",
        "  - Se divide el dataset completo entre 5 subsets (folds), en este caso cada fold tiene aproximadamente 2000 carros.\n",
        "  - Al final obtenemos 5 scores, una por cada subset, esto en toeria refleja que tan bien se comportaria el pipeline si fuera un \"nuevo\" dataset. El promedio de esas 5 scores es lo que llamamos mean macro-F1."
      ],
      "metadata": {
        "id": "53QQETV7uJtP"
      },
      "id": "53QQETV7uJtP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores estan arribe de 0.90, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion.\n",
        "- La media macro-F1 de 0.9274 es el mejor resumen numérico del \"rendimiento esperado\" si se entrenó con el 80 % del conjunto de datos y se probó con el 20 % restante. Esto sugiere que, en promedio, el modelo clasifica correctamente casi todos los ejemplos de \"Media\" y también funciona bien con \"Alta\" y \"Baja\"."
      ],
      "metadata": {
        "id": "0iNV3UqJyHRt"
      },
      "id": "0iNV3UqJyHRt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression Training & Testing\n"
      ],
      "metadata": {
        "id": "TydGfRBNdysi"
      },
      "id": "TydGfRBNdysi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1\n",
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Asegurarse de las columnas presentes en X_train\n",
        "print(\"Columns in X_train:\", X_train.columns.tolist())\n",
        "\n",
        "# Fit X_train, y_train (80%) y evaluar en X_test, y_test (20%)\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_test_pred = lr_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el test\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Logistic Regression Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nLogistic Regression Test Classification Report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_test_pred,\n",
        "    digits=4,         # Numero de decimales\n",
        "    zero_division=0   # Ignorar zero-division errors (debido a strings)\n",
        "))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-xrcNyzzVIZ",
        "outputId": "7413004e-a336-4df6-86ac-e2ae1dad8363"
      },
      "id": "1-xrcNyzzVIZ",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in X_train: ['year', 'selling_price', 'km_driven', 'fuel', 'combustible_estimado_l', 'seller_type', 'transmission', 'owner', 'tipo_carroceria', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Test Accuracy: 0.9615\n",
            "\n",
            "Logistic Regression Test Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.8714    1.0000    0.9313       210\n",
            "        Baja     0.7051    1.0000    0.8271       110\n",
            "       Media     1.0000    0.9542    0.9765      1680\n",
            "\n",
            "    accuracy                         0.9615      2000\n",
            "   macro avg     0.8588    0.9847    0.9116      2000\n",
            "weighted avg     0.9703    0.9615    0.9636      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1603   31   46]\n",
            " [   0  210    0]\n",
            " [   0    0  110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2\n",
        "# 3.3.1 New random split\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=99\n",
        ")\n",
        "\n",
        "# 3.3.2 Re‐fit your original pipeline (with score_calidad)\n",
        "lr_pipeline.fit(X2_train, y2_train)\n",
        "y2_pred = lr_pipeline.predict(X2_test)\n",
        "\n",
        "acc2 = accuracy_score(y2_test, y2_pred)\n",
        "print(f\"New split (seed=99) Test Accuracy: {acc2:.4f}\")\n",
        "print(\"\\nNew split Classification Report:\\n\")\n",
        "print(classification_report(y2_test, y2_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y2_test, y2_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNQUsmC5HPc8",
        "outputId": "824b6a7b-7206-4ab8-8c26-8c34d71348cf"
      },
      "id": "TNQUsmC5HPc8",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New split (seed=99) Test Accuracy: 0.9670\n",
            "\n",
            "New split Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.8678    1.0000    0.9292       210\n",
            "        Baja     0.7639    1.0000    0.8661       110\n",
            "       Media     1.0000    0.9607    0.9800      1680\n",
            "\n",
            "    accuracy                         0.9670      2000\n",
            "   macro avg     0.8772    0.9869    0.9251      2000\n",
            "weighted avg     0.9731    0.9670    0.9684      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1614   32   34]\n",
            " [   0  210    0]\n",
            " [   0    0  110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Logistic Regression"
      ],
      "metadata": {
        "id": "oQOPPp6yTVgh"
      },
      "id": "oQOPPp6yTVgh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input entry test\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "X.columns.tolist()\n",
        "\n",
        "new_car_data = {\n",
        "    \"year\": [2018],                   # int\n",
        "    \"selling_price\": [350000],        # int or float\n",
        "    \"km_driven\": [25000],             # int\n",
        "    \"fuel\": [1],                      # must match the same category codes you used\n",
        "    \"combustible_estimado_l\": [1800.0],  # float\n",
        "    \"seller_type\": [0],               # category code\n",
        "    \"transmission\": [1],              # category code\n",
        "    \"owner\": [0],                     # category code\n",
        "    \"tipo_carroceria\": [2],           # category code\n",
        "    \"potencia_motor_hp\": [120],       # int\n",
        "    \"nivel_seguridad\": [4.5],         # float\n",
        "    \"score_calidad\": [5.5],           # float\n",
        "    \"eficiencia_km_l\": [18.0]         # float\n",
        "    # \"brand\": [\"Toyota\"]             # string; will be cast to category below\n",
        "}\n",
        "\n",
        "new_car_df = pd.DataFrame(new_car_data)\n",
        "\n",
        "cat_cols = [\"fuel\", \"seller_type\", \"transmission\", \"owner\", \"tipo_carroceria\"]\n",
        "for col in cat_cols:\n",
        "    new_car_df[col] = new_car_df[col].astype(\"category\")\n",
        "\n",
        "# new_car_df[\"brand\"] = new_car_df[\"brand\"].astype(\"category\")\n",
        "\n",
        "print(new_car_df.dtypes)\n",
        "\n",
        "predicted_label = lr_pipeline.predict(new_car_df)\n",
        "print(\"\\nPredicted calidad_auto:\", predicted_label[0])\n",
        "\n",
        "predicted_proba = lr_pipeline.predict_proba(new_car_df)\n",
        "print(\"Predicted probabilities [Alta, Media, Baja]:\", predicted_proba[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erwroz9ed6i4",
        "outputId": "b45f8d5a-6637-4748-ea13-059ed61f3e93"
      },
      "id": "Erwroz9ed6i4",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year                         int64\n",
            "selling_price                int64\n",
            "km_driven                    int64\n",
            "fuel                      category\n",
            "combustible_estimado_l     float64\n",
            "seller_type               category\n",
            "transmission              category\n",
            "owner                     category\n",
            "tipo_carroceria           category\n",
            "potencia_motor_hp            int64\n",
            "nivel_seguridad            float64\n",
            "score_calidad              float64\n",
            "eficiencia_km_l            float64\n",
            "dtype: object\n",
            "\n",
            "Predicted calidad_auto: Alta\n",
            "Predicted probabilities [Alta, Media, Baja]: [9.99999983e-01 1.84330860e-30 1.73430678e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n",
        "- Random Forest es un algoritmo de aprendizaje supervisado en machine learning que utiliza múltiples árboles de decisión para clasificar o predecir datos. Es un método de conjunto (ensemble method) que mejora la precisión de las predicciones combinando los resultados de varios modelos débiles (árboles de decisión)."
      ],
      "metadata": {
        "id": "GH-jnMM-gSKE"
      },
      "id": "GH-jnMM-gSKE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Construir e inicializar un RandomForestClassifer\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=300,     # Numero de arboles en el bosque\n",
        "    max_depth=None,       # Profundidad maxima de los arboles\n",
        "    class_weight=\"balanced\",  # Ajuste de pesos para clases desbalanceadas\n",
        "    min_samples_split=2,  # Minimo numero de muestras requeridas para dividir\n",
        "    random_state=42,      # Seed\n",
        "    n_jobs=-1             # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "# Crear un Pipeline unico que aplica el preprocessor previamente definido\n",
        "rf_pipeline = Pipeline([\n",
        "    # Usando el preprocessor que definimos previamente (ColumnTransformer)\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", rf_clf)\n",
        "])"
      ],
      "metadata": {
        "id": "58rnhuodgpHZ"
      },
      "id": "58rnhuodgpHZ",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `n_estimators=300` construye 300 arbholes de decision\n",
        "- `class_weight=\"balanced\"` garantiza que las clases minoritarias tengan mayor ponderacion durante las divisiones\n",
        "- `n_jobs=-1` paraleliza la construccion de arboles en todos los nucleos\n",
        "- Reutilizamos el mismo preprocesador que gestiona la imputacion de la mediana, el escalado y el One-hot encoding. De esta forma, Random Forest ve la misma matriz numerica de 28 columnnas.."
      ],
      "metadata": {
        "id": "FlPbXasikWaP"
      },
      "id": "FlPbXasikWaP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar las columnas consideradas\n",
        "print(\"Features in X:\", X.columns.tolist())\n",
        "\n",
        "# if \"score_calidad\" in X.columns:\n",
        "#     X = X.drop(columns=[\"score_calidad\"])\n",
        "# if \"calidad_code\" in X.columns:\n",
        "#     X = X.drop(columns=[\"calidad_code\"])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el Random Forest\n",
        "rf_cv_scores = cross_val_score(\n",
        "    rf_pipeline,\n",
        "    X,            # full feature set\n",
        "    y,            # full labels\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Imprimir scores del 5-fold test\n",
        "print(\"Random Forest 5-fold CV macro-F1 scores:\", rf_cv_scores.round(4))\n",
        "print(\"Mean macro-F1 (RF):\", rf_cv_scores.mean().round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJEf46_lHaE",
        "outputId": "1992f6ba-9803-4a62-9551-9f22bb74a35c"
      },
      "id": "4tJEf46_lHaE",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features in X: ['year', 'selling_price', 'km_driven', 'fuel', 'combustible_estimado_l', 'seller_type', 'transmission', 'owner', 'tipo_carroceria', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n",
            "Random Forest 5-fold CV macro-F1 scores: [0.7811 0.8864 0.8869 0.885  0.8669]\n",
            "Mean macro-F1 (RF): 0.8613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores son relativamente altos, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion. Sinembargo, parece que el modelo es algo sensible dependeindo de cual 20% es escogido.\n",
        "- Comparado a al modelo de Regresion Logistica, que tuvo un macro-F1 de 0.909 (ignorando la columna \"score_calidad\"), el modelo de Random Forest de hecho so comporta un poco peor en el 5-fold test. Esto puede significar que los limites lineales aprendidos por la regresion logistica eran mas precisos al momento de separar \"Alta\"/\"Media\"/\"Baja\" que el Random Forest."
      ],
      "metadata": {
        "id": "yRE3aAuU3Jbv"
      },
      "id": "yRE3aAuU3Jbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Training and Testing"
      ],
      "metadata": {
        "id": "lhxAYLfVnMj_"
      },
      "id": "lhxAYLfVnMj_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Random Forest pipeline\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el split de 20%\n",
        "y_rf_pred = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "rf_test_acc = accuracy_score(y_test, y_rf_pred)\n",
        "print(f\"Random Forest Test Accuracy: {rf_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nRandom Forest Classification Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_rf_pred,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "rf_cm = confusion_matrix(y_test, y_rf_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", rf_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuip_xlsnAUV",
        "outputId": "13b4ee85-df5d-42a1-c4a4-a93f28dfbbb0"
      },
      "id": "Nuip_xlsnAUV",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Test Accuracy: 0.9555\n",
            "\n",
            "Random Forest Classification Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9448    0.8143    0.8747       210\n",
            "        Baja     0.9605    0.6636    0.7849       110\n",
            "       Media     0.9564    0.9923    0.9740      1680\n",
            "\n",
            "    accuracy                         0.9555      2000\n",
            "   macro avg     0.9539    0.8234    0.8779      2000\n",
            "weighted avg     0.9554    0.9555    0.9532      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1667   10    3]\n",
            " [  39  171    0]\n",
            " [  37    0   73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test 2\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=99\n",
        ")\n",
        "rf_clf2 = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_pipeline2 = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", rf_clf2)\n",
        "])\n",
        "rf_pipeline2.fit(X2_train, y2_train)\n",
        "y2_pred = rf_pipeline2.predict(X2_test)\n",
        "acc2 = accuracy_score(y2_test, y2_pred)\n",
        "print(\"New-split Test Accuracy:\", acc2)\n",
        "print(classification_report(y2_test, y2_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "rf_cm = confusion_matrix(y2_test, y2_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", rf_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h27-Swu2sn5G",
        "outputId": "6556e053-025d-49cf-c3b5-5b0f1f21f9b7"
      },
      "id": "h27-Swu2sn5G",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New-split Test Accuracy: 0.9545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9389    0.8048    0.8667       210\n",
            "        Baja     0.9733    0.6636    0.7892       110\n",
            "       Media     0.9553    0.9923    0.9734      1680\n",
            "\n",
            "    accuracy                         0.9545      2000\n",
            "   macro avg     0.9558    0.8202    0.8764      2000\n",
            "weighted avg     0.9546    0.9545    0.9521      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1667   11    2]\n",
            " [  41  169    0]\n",
            " [  37    0   73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosted Tree (LightGBM)\n",
        "- LightGBM es un marco de trabajo de refuerzo de gradiente rápido, distribuido y de alto rendimiento que utiliza un algoritmo de aprendizaje basado en árboles. Está diseñado específicamente para grandes conjuntos de datos y datos de alta dimensión, y ofrece ventajas como mayor velocidad de entrenamiento, menor uso de memoria y mayor precisión en comparación con otros algoritmos de refuerzo. Los árboles reforzados, en general, son un método de aprendizaje conjunto que combina múltiples aprendices débiles (como árboles de decisión) para crear un modelo predictivo sólido."
      ],
      "metadata": {
        "id": "YucJcX9L5fpB"
      },
      "id": "YucJcX9L5fpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un pipeline de LGBM\n",
        "lgbm_clf = lgb.LGBMClassifier(\n",
        "    objective=\"multiclass\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm_pipeline = Pipeline([\n",
        "    # Usando el preprocessor que definimos previamente (ColumnTransformer)\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", lgbm_clf)\n",
        "])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el Random Forest\n",
        "lgbm_cv_scores = cross_val_score(\n",
        "    lgbm_pipeline,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Imprimir scores del 5-fold test\n",
        "print(\"LightGBM 5-fold CV macro-F1:\", lgbm_cv_scores.round(4))\n",
        "print(\"Mean macro-F1 (LGBM):\", lgbm_cv_scores.mean().round(4))"
      ],
      "metadata": {
        "id": "tuKhuqdh6Pc7",
        "outputId": "a75997ea-7f72-4171-8f5f-43001f716025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tuKhuqdh6Pc7",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM 5-fold CV macro-F1: [0.941  0.9337 0.94   0.9314 0.9213]\n",
            "Mean macro-F1 (LGBM): 0.9335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores son muy altos, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion. El pipeline generaliza muy bien cada 20% del dataset\n",
        "- Este modelo representa una mejora ante los dos modelos implementados previamente\n",
        "  - Regresion Logistica (sin \"score_calidad\"): media CV macro-F1 ≈ 0.909\n",
        "  - Random Forest (sin \"score_calidad\"): media CV macro-F1 ≈ 0.8613\n",
        "- Hasta este punto el modelo de LightGBM tiene el mejor rendimiento categorizando el dataset y prediciendo \"calidad_auto\""
      ],
      "metadata": {
        "id": "sDfrYaVlC_NH"
      },
      "id": "sDfrYaVlC_NH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LightGBM testing"
      ],
      "metadata": {
        "id": "rGl8oDi381Dg"
      },
      "id": "rGl8oDi381Dg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit LGBM pipeline\n",
        "lgbm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el split de 20%\n",
        "y_lgbm_pred = lgbm_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "lgbm_test_acc = accuracy_score(y_test, y_lgbm_pred)\n",
        "print(f\"\\nLightGBM Test Accuracy: {lgbm_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nLightGBM Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_lgbm_pred,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "lgbm_cm = confusion_matrix(y_test, y_lgbm_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", lgbm_cm)"
      ],
      "metadata": {
        "id": "SFZBU2cw84HN",
        "outputId": "2d1c0bae-0ebc-4ae4-9852-07c871dbcd33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SFZBU2cw84HN",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1372\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "\n",
            "LightGBM Test Accuracy: 0.9755\n",
            "\n",
            "LightGBM Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9302    0.9524    0.9412       210\n",
            "        Baja     0.8583    0.9364    0.8957       110\n",
            "       Media     0.9898    0.9810    0.9854      1680\n",
            "\n",
            "    accuracy                         0.9755      2000\n",
            "   macro avg     0.9261    0.9566    0.9407      2000\n",
            "weighted avg     0.9763    0.9755    0.9758      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1648   15   17]\n",
            " [  10  200    0]\n",
            " [   7    0  103]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "bVOp-fRECGuq"
      },
      "id": "bVOp-fRECGuq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yR20KEufHb9I"
      },
      "id": "yR20KEufHb9I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}