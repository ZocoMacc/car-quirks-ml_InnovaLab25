{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & Settings\n",
        "- Descargar e importar dependencias"
      ],
      "metadata": {
        "id": "ryKe98J-Bu0M"
      },
      "id": "ryKe98J-Bu0M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dummy baseline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_squared_error\n",
        "\n",
        "# Preprocessing (Pipelines)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Modeling: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Modeling: Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Modeling: LightGBM\n",
        "# Suprimir warnings (LGBM suele ser sucio con las warnings)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Unknown parameter.*\")\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Modeling: XGBoost\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "eE1_V_wWBq4H",
        "outputId": "fe58cab1-69dd-408a-c47e-f402105ec083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eE1_V_wWBq4H",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "- Cargar el data set\n",
        "- Primer vistazo a la distribucion"
      ],
      "metadata": {
        "id": "QctRhgn6B40Z"
      },
      "id": "QctRhgn6B40Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get url from file hosted on GitHub\n",
        "url = \"https://raw.githubusercontent.com/ZocoMacc/car-quirks-ml_InnovaLab25/refs/heads/main/data/cars_data.csv\"\n",
        "\n",
        "# Cargar csv en un DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Explorar la estructura del DataFrame (Sanity Check)\n",
        "df.info()\n",
        "df.shape\n",
        "# df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzqNHCXUCZTN",
        "outputId": "c7cd059e-0938-4d83-8e18-942ed68c347c"
      },
      "id": "EzqNHCXUCZTN",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   name                    10000 non-null  object \n",
            " 1   year                    10000 non-null  int64  \n",
            " 2   selling_price           10000 non-null  int64  \n",
            " 3   km_driven               10000 non-null  int64  \n",
            " 4   fuel                    10000 non-null  int64  \n",
            " 5   combustible_estimado_l  10000 non-null  float64\n",
            " 6   seller_type             10000 non-null  int64  \n",
            " 7   transmission            10000 non-null  int64  \n",
            " 8   owner                   10000 non-null  int64  \n",
            " 9   tipo_carroceria         10000 non-null  int64  \n",
            " 10  potencia_motor_hp       10000 non-null  int64  \n",
            " 11  nivel_seguridad         10000 non-null  float64\n",
            " 12  calidad_auto            10000 non-null  object \n",
            " 13  score_calidad           10000 non-null  float64\n",
            " 14  eficiencia_km_l         10000 non-null  float64\n",
            "dtypes: float64(4), int64(9), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el balance de las clases (84% Media, 10.5% Alta, 5.5% Baja)\n",
        "df[\"calidad_auto\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "tfZjya5Ya2MI",
        "outputId": "08fb76ca-a6ac-4bce-eeb0-4c9a8545cba3"
      },
      "id": "tfZjya5Ya2MI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "calidad_auto\n",
              "Media    0.8399\n",
              "Alta     0.1052\n",
              "Baja     0.0549\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>calidad_auto</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Media</th>\n",
              "      <td>0.8399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Alta</th>\n",
              "      <td>0.1052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Baja</th>\n",
              "      <td>0.0549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- Un fuerte desbalanceo de clases\n",
        "  - \"Media\" es la mayoria con un 84%\n",
        "  - \"Alta\" solo cubre el 10%, y \"Baja\" solo un 5.5%\n",
        "- El accuracy de referencia (baseline) es engañosamente alto\n",
        "  - Un DummyClassifier que siempre predice \"Media\" deberia de poder predecir 83.99%.\n",
        "  - Superar este baseline no garantiza que el modelo sea mas precizo detectando \"Alta\" o \"Baja\".\n",
        "- Riesgo de abandono de las clases minoritarias\n",
        "  - El modelo tiene el riesgo de predecir \"Media\" la mayoria del tiempo debido al desbalanceo de las clases, lo que podria ignorar \"Alta\" o \"Baja\" como opciones."
      ],
      "metadata": {
        "id": "4s-HoOX8IJAh"
      },
      "id": "4s-HoOX8IJAh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning & Dropping\n",
        "- Decidir las variables que seran ignoradas por el modelo.\n",
        "- Si es necesarion, transformar valores en interpretaciones legibles por el modelo o marcarlas con una categoria."
      ],
      "metadata": {
        "id": "EujHrAybfup-"
      },
      "id": "EujHrAybfup-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Duda sobre la variable \"score_calidad\": Que tanto se correlaciona con \"calidad_auto\"\n",
        "# Mapear las categorias con numeros para calcular la correlacion\n",
        "mapping = {\"Baja\": 0, \"Media\": 1, \"Alta\": 2}\n",
        "df[\"calidad_code\"] = df[\"calidad_auto\"].map(mapping)\n",
        "\n",
        "# Calcular la correlacion\n",
        "corr = df[[\"score_calidad\", \"calidad_code\"]].corr().iloc[0, 1]\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Correlacion entre score_calidad y calidad_auto: {corr:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA-E2kkjgLhv",
        "outputId": "527ae23b-e35c-43cb-af1f-58d4128f2b5a"
      },
      "id": "kA-E2kkjgLhv",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlacion entre score_calidad y calidad_auto: 0.740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- La correlacion entre \"score_calidad\" y \"calidad_auto\" no es 1.0, lo que quiere decir que \"calidad_auto\" no depende al 100% de la variable \"score_calidad\".\n",
        "- La variable \"score_calidad\" va a ser considerada."
      ],
      "metadata": {
        "id": "ZTNtSbS-hsXo"
      },
      "id": "ZTNtSbS-hsXo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extraer marca del nombre\n",
        "- Usar el nombre y cortarlo de una manera en la que podamos extrar la marca del auto para poder usarlo como variable y ser considerada."
      ],
      "metadata": {
        "id": "W7UU6SNHlHoE"
      },
      "id": "W7UU6SNHlHoE"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Rifense este rollo porfi\n",
        "- Extraer el nombre de la marca de cada carro (crar una columna para \"brand\")\n",
        "- Investigar si es viable extraer tambien el submodelo del carro (Limited, Sport, AC, ZX, etc)\n",
        "  - Maybe no nos conviene por que son un chingo pero maybe si si no son tantas quien sabe\n",
        "  - Si si es viable crear una columna para \"submodel\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-0SgRIRHmF5z",
        "outputId": "67941b3a-ac7f-4b0f-e5b5-8c085fad1074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "-0SgRIRHmF5z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRifense este rollo porfi\\n- Extraer el nombre de la marca de cada carro (crar una columna para \"brand\")\\n- Investigar si es viable extraer tambien el submodelo del carro (Limited, Sport, AC, ZX, etc)\\n  - Maybe no nos conviene por que son un chingo pero maybe si si no son tantas quien sabe\\n  - Si si es viable crear una columna para \"submodel\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convertir columnas de enteros en categorias\n",
        "- Columnas como \"fuel\", \"seller_type\", \"transmission\", \"owner\", \"tipo_carroceria\" tienen asignado un valor entero pero en realidad representan categorias sin orden.\n",
        "- Convertir estas variables a Pandas' category dtype para mas claridad y eficiencia en la manipulacion de estas variables, tambien es mas seguro hacerlo de esta forma, asi evitamos que estas columnas se interpreten como numeric imputers o scalers."
      ],
      "metadata": {
        "id": "43A-JGJKnxf2"
      },
      "id": "43A-JGJKnxf2"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_int_cols = [\n",
        "    \"fuel\",\n",
        "    \"seller_type\",\n",
        "    \"transmission\",\n",
        "    \"owner\",\n",
        "    \"tipo_carroceria\"\n",
        "    #\"brand\"             # categoria extraida del nombre\n",
        "]\n",
        "\n",
        "for col in cat_int_cols:\n",
        "    df[col] = df[col].astype(\"category\")"
      ],
      "metadata": {
        "id": "CZlrE_CHn9ZP"
      },
      "id": "CZlrE_CHn9ZP",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Droppear columnas innecesarias\n",
        "- Calcular correlaciones entre las columnas para identificar cual tiene un gran efecto en \"calidad_auto\"\n",
        "- Definir X, y para entrenamiento"
      ],
      "metadata": {
        "id": "xJL2hxc2NKNk"
      },
      "id": "xJL2hxc2NKNk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Al final se droppea el nombre (ya habiendo extraido la brand)\n",
        "df = df.drop(columns=[\"name\"])\n",
        "\n",
        "# Ver correlaciones para identificar leaks\n",
        "correlations = df.drop(columns=[\"calidad_auto\"]).corrwith(df[\"calidad_code\"])\n",
        "print(\"Correlaciones\")\n",
        "print(correlations.sort_values(ascending=False))\n",
        "\n",
        "# Tirar la columna helper que definimos previemente\n",
        "df = df.drop(columns=[\"calidad_code\"])\n",
        "\n",
        "print(\"\\nColumnas que permanecen en el DataFrame:\")\n",
        "print(df.dtypes)  # Checar las columnas que permanecen\n",
        "\n",
        "# Definir X & y para el training\n",
        "features_to_drop = [\"calidad_auto\", \"score_calidad\", \"combustible_estimado_l\", \"owner\", \"seller_type\", \"fuel\", \"selling_price\", \"transmission\"]\n",
        "X = df.drop(columns=features_to_drop)\n",
        "y = df[\"calidad_auto\"]    # Estableciendo el target\n",
        "\n",
        "# Inspeccionar las variables que permanecen\n",
        "# print(\"Columnas a considerar:\\n\", X.columns.tolist())\n",
        "print(\"\\nFeature dtypes (para X):\")\n",
        "print(X.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te-8G4rdNOT0",
        "outputId": "6cff513b-903f-4e02-d419-be177db21533"
      },
      "id": "te-8G4rdNOT0",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlaciones\n",
            "calidad_code              1.000000\n",
            "score_calidad             0.740022\n",
            "eficiencia_km_l           0.526133\n",
            "nivel_seguridad           0.497154\n",
            "year                      0.495617\n",
            "potencia_motor_hp         0.294303\n",
            "transmission              0.197374\n",
            "selling_price             0.103824\n",
            "fuel                      0.031103\n",
            "seller_type               0.028950\n",
            "km_driven                -0.024826\n",
            "tipo_carroceria          -0.033271\n",
            "owner                    -0.045147\n",
            "combustible_estimado_l   -0.107143\n",
            "dtype: float64\n",
            "\n",
            "Columnas que permanecen en el DataFrame:\n",
            "year                         int64\n",
            "selling_price                int64\n",
            "km_driven                    int64\n",
            "fuel                      category\n",
            "combustible_estimado_l     float64\n",
            "seller_type               category\n",
            "transmission              category\n",
            "owner                     category\n",
            "tipo_carroceria           category\n",
            "potencia_motor_hp            int64\n",
            "nivel_seguridad            float64\n",
            "calidad_auto                object\n",
            "score_calidad              float64\n",
            "eficiencia_km_l            float64\n",
            "dtype: object\n",
            "\n",
            "Feature dtypes (para X):\n",
            "year                    int64\n",
            "km_driven               int64\n",
            "tipo_carroceria      category\n",
            "potencia_motor_hp       int64\n",
            "nivel_seguridad       float64\n",
            "eficiencia_km_l       float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DummyClassifier & Baseline\n",
        "- El primer paso es establecer una baseline y asegurarse de que la informacion esta limpia\n",
        "- El objetivo es confirmar ques este DummyClassifier de como resultado una precision de 83.99%\n",
        "- Se usara la estrategia de \"most_frequent\", simplemente el valor que mas se repite sera elegido para la prediccion.\n",
        "- Este modelo representa el piso (baseline) de qualquier modelo futuro.\n",
        "- El split para entrenamiento es un 80/20 asegurandose de que cada clase aparece con las misma proporciones.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ng1LdD5yB6_t"
      },
      "id": "Ng1LdD5yB6_t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Test\n",
        "# print(\"Train class proportions:\\n\", y_train.value_counts(normalize=True))\n",
        "# print(\"\\nTest class proportions:\\n\", y_test.value_counts(normalize=True))\n",
        "\n",
        "# Iniciar DummyClassifier\n",
        "dummy = DummyClassifier(\n",
        "    strategy=\"most_frequent\", # Siempre predice la clase mas frecuente (\"Media\")\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dummy.fit(X_train, y_train)     # \"Entrena\" basado en la clase mas frecuente\n",
        "\n",
        "# Obtener predicciones\n",
        "y_pred = dummy.predict(X_test)  # Aplica la regla a cada fila de X_test\n",
        "\n",
        "# Evaluar accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)   # Fraccion de predicciones correctas\n",
        "print(f\"DummyClassifier Accuracy: {accuracy}\")\n",
        "\n",
        "# Visualizar resulatdos de predicciones\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    digits=4,         # Numero de decimales\n",
        "    zero_division=0   # Ignorar zero-division errors (debido a strings)\n",
        "))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC8kJNPZM2Xt",
        "outputId": "4e96e3a5-4f2c-4c79-f74a-41e9838fbbbe"
      },
      "id": "WC8kJNPZM2Xt",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier Accuracy: 0.84\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.0000    0.0000    0.0000       210\n",
            "        Baja     0.0000    0.0000    0.0000       110\n",
            "       Media     0.8400    1.0000    0.9130      1680\n",
            "\n",
            "    accuracy                         0.8400      2000\n",
            "   macro avg     0.2800    0.3333    0.3043      2000\n",
            "weighted avg     0.7056    0.8400    0.7670      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1680    0    0]\n",
            " [ 210    0    0]\n",
            " [ 110    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lo que esto nos dice\n",
        "- Tenemos un baseline de ~84% accuracy, este es el objetivo a vencer en los proximos modelos."
      ],
      "metadata": {
        "id": "wgRyZpmLYjPP"
      },
      "id": "wgRyZpmLYjPP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "- Preparar el data set para ser procesado\n",
        "- Dividir entre variables numericas y categoricas\n",
        "- Implementar un pipeline para cada tipo de variable\n",
        "- Usar ColumnTransformer para aplicar los pipelines al DataFrame"
      ],
      "metadata": {
        "id": "PSOmYyV3CAqt"
      },
      "id": "PSOmYyV3CAqt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identificar columnas en base al dtype"
      ],
      "metadata": {
        "id": "zx671--0xzfX"
      },
      "id": "zx671--0xzfX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos numericos\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "print(\"Numeric features:\", num_cols)\n",
        "\n",
        "# Datos categoricos\n",
        "cat_cols = X.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "print(\"\\nCategorical features:\", cat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU_JcFDaxLl4",
        "outputId": "ca485889-cd2f-41c8-9147-c20087c709c1"
      },
      "id": "kU_JcFDaxLl4",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features: ['year', 'km_driven', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n",
            "\n",
            "Categorical features: ['tipo_carroceria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Construir el pipeline numerico"
      ],
      "metadata": {
        "id": "4h0v9hENyWIX"
      },
      "id": "4h0v9hENyWIX"
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipe = Pipeline([\n",
        "    # Definiendo tecnicas de defensive programming\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")), # Imputacion de media\n",
        "    (\"scaler\", StandardScaler())                   # Standard scaling\n",
        "])"
      ],
      "metadata": {
        "id": "vepTzJLPyYXG"
      },
      "id": "vepTzJLPyYXG",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El objetivo de la funcion `SimpleImputer()` es remplazar valores faltantes por el valor definido por su argumento (e.g. median).\n",
        "- El objetivo de la funcion `StandardScaler()` es que despues de una imputacion, el valor numerico es escalado para tener una media = 0 y standard deviation = 1. De esta forma se evitan posibles errores generados por tener diferentes escalas de numeros en las diferentes columnas."
      ],
      "metadata": {
        "id": "cyk6t5x1y9ad"
      },
      "id": "cyk6t5x1y9ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Construir el pipeline categorico"
      ],
      "metadata": {
        "id": "A-TcmKaA0xQd"
      },
      "id": "A-TcmKaA0xQd"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipe = Pipeline([\n",
        "    # Definiendo tecnicas de defensive programming\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Imputacion de moda\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))    # One-hot encoding\n",
        "])"
      ],
      "metadata": {
        "id": "5WypSVy705LP"
      },
      "id": "5WypSVy705LP",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `SimpleImputer()` remplaza cualquier valor faltante por el valor mas frecuente (la moda). Esto preserva la distribucion del data set.\n",
        "- El objetivo del OneHotEncoder es convertir cada columna categorica en las columnas binarias necesarias para representar cada valor categorico unico.\n",
        "  - `handle_unkwnown=\"ignore\"` es util para que en caso de que aparezca una nueva categoria \"vacia\", que esta sea representada por una columna entera de puros ceros."
      ],
      "metadata": {
        "id": "3EmHAM6wBJWM"
      },
      "id": "3EmHAM6wBJWM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementacion de ColumnTransformer para aplicar los pipelines"
      ],
      "metadata": {
        "id": "KlPpE_hsE6D_"
      },
      "id": "KlPpE_hsE6D_"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Aplicar num_pipe a las columnas numericas\n",
        "        (\"nums\", num_pipe, num_cols),\n",
        "        # Aplicar cat_pipe a columnas categoricas\n",
        "        (\"cats\", cat_pipe, cat_cols)\n",
        "    ],\n",
        "    remainder=\"drop\" # Tirar cualquier columna no listada\n",
        ")"
      ],
      "metadata": {
        "id": "a1xq23TBFJSe"
      },
      "id": "a1xq23TBFJSe",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `transformers` es un array de tuples `(name, pipeline, columns)` que ira siendo populado mientras `.fit()` corre\n",
        "  1. `(\"nums\", num_pipe, num_cols)` toma todas las columnas en `num_cols` y aplica el pipeline `num_pipe` y entraga como output el array numerico transformado.\n",
        "  2. `(\"cats\", cat_pipe, cat_cols)` toma todas las columnas en `cat_cols` y aplica el pipeline `cat_pipe` y entraga como output el array categorico transformado."
      ],
      "metadata": {
        "id": "kl43BKMVJVR3"
      },
      "id": "kl43BKMVJVR3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspeccionar el output transformado"
      ],
      "metadata": {
        "id": "q3596Gl4KnfD"
      },
      "id": "q3596Gl4KnfD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Testear el preprocessor\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Ver la forma del array final\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "print(\"Transformed shape:\", X_train_transformed.shape)\n",
        "#print(len(num_cols))\n",
        "\n",
        "# Imprimir las columnas finales\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(\"All output features:\\n\", feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7F5TjTMKufV",
        "outputId": "ebbebb36-c2be-442e-c94d-a7d192d801cf"
      },
      "id": "z7F5TjTMKufV",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed shape: (8000, 10)\n",
            "All output features:\n",
            " ['nums__year' 'nums__km_driven' 'nums__potencia_motor_hp'\n",
            " 'nums__nivel_seguridad' 'nums__eficiencia_km_l' 'cats__tipo_carroceria_1'\n",
            " 'cats__tipo_carroceria_2' 'cats__tipo_carroceria_3'\n",
            " 'cats__tipo_carroceria_4' 'cats__tipo_carroceria_5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que esto nos dice\n",
        "- En total hay 28 columnas ya con los pipelines aplicados (8 numericas y 20 categoricas) - temporal (falta agregar \"brand\" y talvez \"submodel\")\n",
        "- Los prefijos `nums_` y `cats_` nos dicen de cual pipeline proviene cada columna\n",
        "- Estos nombres nos serviran despues para implementacion de funciones o debugging"
      ],
      "metadata": {
        "id": "SZumUwvGMuMR"
      },
      "id": "SZumUwvGMuMR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "- La regresión logística es una técnica de análisis de datos que utiliza las matemáticas para encontrar las relaciones entre dos factores de datos. Luego, utiliza esta relación para predecir el valor de uno de esos factores basándose en el otro. Normalmente, la predicción tiene un número finito de resultados, como un sí o un no."
      ],
      "metadata": {
        "id": "H2t9M4f-R5j2"
      },
      "id": "H2t9M4f-R5j2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un objecto de LogisticRegression\n",
        "logreg = LogisticRegression(\n",
        "    multi_class=\"multinomial\",   # Clasificacion multiclase\n",
        "    solver=\"saga\",               # Algoritmo de optimizacion (soporta L1/L2)\n",
        "    class_weight=\"balanced\",     # Ajuste de pesos para clases desbalanceadas\n",
        "    max_iter=1000,               # Numero maximo de iteraciones\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Construir un unico pipeline\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", logreg)\n",
        "])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el modelo de regresion logistica\n",
        "cv_macro_f1 = cross_val_score(\n",
        "    lr_pipeline,    # Pipeline completeo\n",
        "    X,              # Todas las features\n",
        "    y,              # Todas las labels\n",
        "    cv=5,           # 5 folds\n",
        "    scoring=\"f1_macro\", # macro-averaged F1\n",
        "    n_jobs=-1       # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "# Imprimir los fold scores y la media\n",
        "print(\"Logistic Regression 5-fold CV macro-F1 scores:\", cv_macro_f1.round(4))\n",
        "print(\"Mean macro-F1:\", cv_macro_f1.mean().round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSvhbZ08R-Ig",
        "outputId": "5514f384-b947-40cf-ec99-e68f10ce76a7"
      },
      "id": "wSvhbZ08R-Ig",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression 5-fold CV macro-F1 scores: [0.9436 0.9194 0.9054 0.8841 0.887 ]\n",
            "Mean macro-F1: 0.9079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entendiendo la media Macro-F1\n",
        "- La evaluacion multiclase Macro-F1 toma el promedio no ponderado de los tres puntajes F1 por clase (\"Alta\", \"Media\", \"Baja\"). Este proceso toma cada clase de forma equivalente aunque la distribucion sea diferente (10.5%, 84%, 5.5%). Usando macro-F1 nos aseguramos de que el modelo no esta eligiendo \"Media\" en todas las predicciones y realmente esta prediciendo entre las tres clases.\n",
        "- El metodo de evaluacion 5-fold cross-validation es una manera de estimar que tan bien el pipeline (preprocessor + modelo) se generaliza a datasets no vistos\n",
        "  - Se divide el dataset completo entre 5 subsets (folds), en este caso cada fold tiene aproximadamente 2000 carros.\n",
        "  - Al final obtenemos 5 scores, una por cada subset, esto en toeria refleja que tan bien se comportaria el pipeline si fuera un \"nuevo\" dataset. El promedio de esas 5 scores es lo que llamamos mean macro-F1."
      ],
      "metadata": {
        "id": "53QQETV7uJtP"
      },
      "id": "53QQETV7uJtP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores estan arribe de 0.90, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion.\n",
        "- La media macro-F1 de 0.9274 es el mejor resumen numérico del \"rendimiento esperado\" si se entrenó con el 80 % del conjunto de datos y se probó con el 20 % restante. Esto sugiere que, en promedio, el modelo clasifica correctamente casi todos los ejemplos de \"Media\" y también funciona bien con \"Alta\" y \"Baja\"."
      ],
      "metadata": {
        "id": "0iNV3UqJyHRt"
      },
      "id": "0iNV3UqJyHRt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression Training & Testing\n"
      ],
      "metadata": {
        "id": "TydGfRBNdysi"
      },
      "id": "TydGfRBNdysi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1\n",
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Asegurarse de las columnas presentes en X_train\n",
        "print(\"Columns in X_train:\", X_train.columns.tolist())\n",
        "\n",
        "# Fit X_train, y_train (80%) y evaluar en X_test, y_test (20%)\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_test_pred = lr_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el test\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Logistic Regression Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nLogistic Regression Test Classification Report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_test_pred,\n",
        "    digits=4,         # Numero de decimales\n",
        "    zero_division=0   # Ignorar zero-division errors (debido a strings)\n",
        "))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-xrcNyzzVIZ",
        "outputId": "565daaed-94c9-48f0-8d2b-da9001c2721e"
      },
      "id": "1-xrcNyzzVIZ",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in X_train: ['year', 'km_driven', 'tipo_carroceria', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n",
            "Logistic Regression Test Accuracy: 0.9590\n",
            "\n",
            "Logistic Regression Test Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.8642    1.0000    0.9272       210\n",
            "        Baja     0.6918    1.0000    0.8178       110\n",
            "       Media     1.0000    0.9512    0.9750      1680\n",
            "\n",
            "    accuracy                         0.9590      2000\n",
            "   macro avg     0.8520    0.9837    0.9067      2000\n",
            "weighted avg     0.9688    0.9590    0.9613      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1598   33   49]\n",
            " [   0  210    0]\n",
            " [   0    0  110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2\n",
        "# 3.3.1 New random split\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=99\n",
        ")\n",
        "\n",
        "# 3.3.2 Re‐fit your original pipeline (with score_calidad)\n",
        "lr_pipeline.fit(X2_train, y2_train)\n",
        "y2_pred = lr_pipeline.predict(X2_test)\n",
        "\n",
        "acc2 = accuracy_score(y2_test, y2_pred)\n",
        "print(f\"New split (seed=99) Test Accuracy: {acc2:.4f}\")\n",
        "print(\"\\nNew split Classification Report:\\n\")\n",
        "print(classification_report(y2_test, y2_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Generar confusion matrix\n",
        "cm = confusion_matrix(y2_test, y2_pred, labels=[\"Media\",\"Alta\",\"Baja\"])\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNQUsmC5HPc8",
        "outputId": "8ff1d023-77ec-4418-c2fc-a7291d37a9eb"
      },
      "id": "TNQUsmC5HPc8",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New split (seed=99) Test Accuracy: 0.9645\n",
            "\n",
            "New split Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.8607    1.0000    0.9251       210\n",
            "        Baja     0.7483    1.0000    0.8560       110\n",
            "       Media     1.0000    0.9577    0.9784      1680\n",
            "\n",
            "    accuracy                         0.9645      2000\n",
            "   macro avg     0.8697    0.9859    0.9199      2000\n",
            "weighted avg     0.9715    0.9645    0.9661      2000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1609   34   37]\n",
            " [   0  210    0]\n",
            " [   0    0  110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Logistic Regression"
      ],
      "metadata": {
        "id": "oQOPPp6yTVgh"
      },
      "id": "oQOPPp6yTVgh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input entry test\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "X.columns.tolist()\n",
        "\n",
        "new_car_data = {\n",
        "    \"year\": [2018],                   # int\n",
        "    \"selling_price\": [350000],        # int or float\n",
        "    \"km_driven\": [25000],             # int\n",
        "    \"fuel\": [1],                      # must match the same category codes you used\n",
        "    \"combustible_estimado_l\": [1800.0],  # float\n",
        "    \"seller_type\": [0],               # category code\n",
        "    \"transmission\": [1],              # category code\n",
        "    \"owner\": [0],                     # category code\n",
        "    \"tipo_carroceria\": [2],           # category code\n",
        "    \"potencia_motor_hp\": [120],       # int\n",
        "    \"nivel_seguridad\": [4.5],         # float\n",
        "    \"score_calidad\": [5.5],           # float\n",
        "    \"eficiencia_km_l\": [18.0]         # float\n",
        "    # \"brand\": [\"Toyota\"]             # string; will be cast to category below\n",
        "}\n",
        "\n",
        "new_car_df = pd.DataFrame(new_car_data)\n",
        "\n",
        "cat_cols = [\"fuel\", \"seller_type\", \"transmission\", \"owner\", \"tipo_carroceria\"]\n",
        "for col in cat_cols:\n",
        "    new_car_df[col] = new_car_df[col].astype(\"category\")\n",
        "\n",
        "# new_car_df[\"brand\"] = new_car_df[\"brand\"].astype(\"category\")\n",
        "\n",
        "print(new_car_df.dtypes)\n",
        "\n",
        "predicted_label = lr_pipeline.predict(new_car_df)\n",
        "print(\"\\nPredicted calidad_auto:\", predicted_label[0])\n",
        "\n",
        "predicted_proba = lr_pipeline.predict_proba(new_car_df)\n",
        "print(\"Predicted probabilities [Alta, Media, Baja]:\", predicted_proba[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erwroz9ed6i4",
        "outputId": "f19ca7ba-5dcf-4fd9-abe9-0cddc4dbfb0f"
      },
      "id": "Erwroz9ed6i4",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year                         int64\n",
            "selling_price                int64\n",
            "km_driven                    int64\n",
            "fuel                      category\n",
            "combustible_estimado_l     float64\n",
            "seller_type               category\n",
            "transmission              category\n",
            "owner                     category\n",
            "tipo_carroceria           category\n",
            "potencia_motor_hp            int64\n",
            "nivel_seguridad            float64\n",
            "score_calidad              float64\n",
            "eficiencia_km_l            float64\n",
            "dtype: object\n",
            "\n",
            "Predicted calidad_auto: Alta\n",
            "Predicted probabilities [Alta, Media, Baja]: [9.99999979e-01 3.33652530e-30 2.08906286e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n",
        "- Random Forest es un algoritmo de aprendizaje supervisado en machine learning que utiliza múltiples árboles de decisión para clasificar o predecir datos. Es un método de conjunto (ensemble method) que mejora la precisión de las predicciones combinando los resultados de varios modelos débiles (árboles de decisión)."
      ],
      "metadata": {
        "id": "GH-jnMM-gSKE"
      },
      "id": "GH-jnMM-gSKE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer split (Train/Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 80/20 split\n",
        "    stratify=y,       # preserva los ratios 84/10.5/5.5 para train y test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Construir e inicializar un RandomForestClassifer\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=300,     # Numero de arboles en el bosque\n",
        "    max_depth=None,       # Profundidad maxima de los arboles\n",
        "    class_weight=\"balanced\",  # Ajuste de pesos para clases desbalanceadas\n",
        "    min_samples_split=2,  # Minimo numero de muestras requeridas para dividir\n",
        "    random_state=42,      # Seed\n",
        "    n_jobs=-1             # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "# Crear un Pipeline unico que aplica el preprocessor previamente definido\n",
        "rf_pipeline = Pipeline([\n",
        "    # Usando el preprocessor que definimos previamente (ColumnTransformer)\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", rf_clf)\n",
        "])"
      ],
      "metadata": {
        "id": "58rnhuodgpHZ"
      },
      "id": "58rnhuodgpHZ",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `n_estimators=300` construye 300 arbholes de decision\n",
        "- `class_weight=\"balanced\"` garantiza que las clases minoritarias tengan mayor ponderacion durante las divisiones\n",
        "- `n_jobs=-1` paraleliza la construccion de arboles en todos los nucleos\n",
        "- Reutilizamos el mismo preprocesador que gestiona la imputacion de la mediana, el escalado y el One-hot encoding. De esta forma, Random Forest ve la misma matriz numerica de 28 columnnas.."
      ],
      "metadata": {
        "id": "FlPbXasikWaP"
      },
      "id": "FlPbXasikWaP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar las columnas consideradas\n",
        "print(\"Features in X:\", X.columns.tolist())\n",
        "\n",
        "# if \"score_calidad\" in X.columns:\n",
        "#     X = X.drop(columns=[\"score_calidad\"])\n",
        "# if \"calidad_code\" in X.columns:\n",
        "#     X = X.drop(columns=[\"calidad_code\"])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el Random Forest\n",
        "rf_cv_scores = cross_val_score(\n",
        "    rf_pipeline,\n",
        "    X,            # full feature set\n",
        "    y,            # full labels\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Imprimir scores del 5-fold test\n",
        "print(\"Random Forest 5-fold CV macro-F1 scores:\", rf_cv_scores.round(4))\n",
        "print(\"Mean macro-F1 (RF):\", rf_cv_scores.mean().round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJEf46_lHaE",
        "outputId": "076d4610-27dc-4359-c834-5d23a584d840"
      },
      "id": "4tJEf46_lHaE",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features in X: ['year', 'km_driven', 'tipo_carroceria', 'potencia_motor_hp', 'nivel_seguridad', 'eficiencia_km_l']\n",
            "Random Forest 5-fold CV macro-F1 scores: [0.8697 0.9214 0.933  0.9267 0.9023]\n",
            "Mean macro-F1 (RF): 0.9106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores son relativamente altos, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion. Sinembargo, parece que el modelo es algo sensible dependeindo de cual 20% es escogido.\n",
        "- Comparado a al modelo de Regresion Logistica, que tuvo un macro-F1 de 0.909 (ignorando la columna \"score_calidad\"), el modelo de Random Forest de hecho so comporta un poco peor en el 5-fold test. Esto puede significar que los limites lineales aprendidos por la regresion logistica eran mas precisos al momento de separar \"Alta\"/\"Media\"/\"Baja\" que el Random Forest."
      ],
      "metadata": {
        "id": "yRE3aAuU3Jbv"
      },
      "id": "yRE3aAuU3Jbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Training and Testing"
      ],
      "metadata": {
        "id": "lhxAYLfVnMj_"
      },
      "id": "lhxAYLfVnMj_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Random Forest pipeline\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el split de 20%\n",
        "y_rf_pred = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "rf_test_acc = accuracy_score(y_test, y_rf_pred)\n",
        "print(f\"Random Forest Test Accuracy: {rf_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nRandom Forest Classification Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_rf_pred,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "rf_cm = confusion_matrix(y_test, y_rf_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", rf_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuip_xlsnAUV",
        "outputId": "f6458297-fbec-4ec9-85cc-0dec15cbe200"
      },
      "id": "Nuip_xlsnAUV",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Test Accuracy: 0.9715\n",
            "\n",
            "Random Forest Classification Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9585    0.8810    0.9181       210\n",
            "        Baja     0.9479    0.8273    0.8835       110\n",
            "       Media     0.9743    0.9923    0.9832      1680\n",
            "\n",
            "    accuracy                         0.9715      2000\n",
            "   macro avg     0.9602    0.9002    0.9283      2000\n",
            "weighted avg     0.9712    0.9715    0.9709      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1667    8    5]\n",
            " [  25  185    0]\n",
            " [  19    0   91]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test 2\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=99\n",
        ")\n",
        "rf_clf2 = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_pipeline2 = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", rf_clf2)\n",
        "])\n",
        "rf_pipeline2.fit(X2_train, y2_train)\n",
        "y2_pred = rf_pipeline2.predict(X2_test)\n",
        "acc2 = accuracy_score(y2_test, y2_pred)\n",
        "print(\"New-split Test Accuracy:\", acc2)\n",
        "print(classification_report(y2_test, y2_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "rf_cm = confusion_matrix(y2_test, y2_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", rf_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h27-Swu2sn5G",
        "outputId": "dc3d8b60-1ac0-4de0-8953-8719991f5f27"
      },
      "id": "h27-Swu2sn5G",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New-split Test Accuracy: 0.972\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9788    0.8810    0.9273       210\n",
            "        Baja     0.9368    0.8091    0.8683       110\n",
            "       Media     0.9732    0.9940    0.9835      1680\n",
            "\n",
            "    accuracy                         0.9720      2000\n",
            "   macro avg     0.9630    0.8947    0.9264      2000\n",
            "weighted avg     0.9718    0.9720    0.9713      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1670    4    6]\n",
            " [  25  185    0]\n",
            " [  21    0   89]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosted Tree (LightGBM)\n",
        "- LightGBM es un marco de trabajo de refuerzo de gradiente rápido, distribuido y de alto rendimiento que utiliza un algoritmo de aprendizaje basado en árboles. Está diseñado específicamente para grandes conjuntos de datos y datos de alta dimensión, y ofrece ventajas como mayor velocidad de entrenamiento, menor uso de memoria y mayor precisión en comparación con otros algoritmos de refuerzo. Los árboles reforzados, en general, son un método de aprendizaje conjunto que combina múltiples aprendices débiles (como árboles de decisión) para crear un modelo predictivo sólido."
      ],
      "metadata": {
        "id": "YucJcX9L5fpB"
      },
      "id": "YucJcX9L5fpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un pipeline de LGBM\n",
        "lgbm_clf = lgb.LGBMClassifier(\n",
        "    objective=\"multiclass\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lgbm_pipeline = Pipeline([\n",
        "    # Usando el preprocessor que definimos previamente (ColumnTransformer)\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", lgbm_clf)\n",
        "])\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar el LightGBM\n",
        "lgbm_cv_scores = cross_val_score(\n",
        "    lgbm_pipeline,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Imprimir scores del 5-fold test\n",
        "print(\"LightGBM 5-fold CV macro-F1:\", lgbm_cv_scores.round(4))\n",
        "print(\"Mean macro-F1 (LGBM):\", lgbm_cv_scores.mean().round(4))"
      ],
      "metadata": {
        "id": "tuKhuqdh6Pc7",
        "outputId": "3cf6124c-6fb8-4e74-dec5-fe6730f932b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tuKhuqdh6Pc7",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM 5-fold CV macro-F1: [0.9451 0.9377 0.9429 0.9257 0.9166]\n",
            "Mean macro-F1 (LGBM): 0.9336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Los 5 scores son muy altos, lo que quiere decir que el pipeline esta consistentemente obteniendo un score alto en \"nueva\" informacion. El pipeline generaliza muy bien cada 20% del dataset\n",
        "- Este modelo representa una mejora ante los dos modelos implementados previamente\n",
        "  - Regresion Logistica (sin \"score_calidad\"): media CV macro-F1 ≈ 0.909\n",
        "  - Random Forest (sin \"score_calidad\"): media CV macro-F1 ≈ 0.8613\n",
        "- Hasta este punto el modelo de LightGBM tiene el mejor rendimiento categorizando el dataset y prediciendo \"calidad_auto\""
      ],
      "metadata": {
        "id": "sDfrYaVlC_NH"
      },
      "id": "sDfrYaVlC_NH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LightGBM testing"
      ],
      "metadata": {
        "id": "rGl8oDi381Dg"
      },
      "id": "rGl8oDi381Dg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit LGBM pipeline en el split de 80%\n",
        "lgbm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el split de 20%\n",
        "y_lgbm_pred = lgbm_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "lgbm_test_acc = accuracy_score(y_test, y_lgbm_pred)\n",
        "print(f\"\\nLightGBM Test Accuracy: {lgbm_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nLightGBM Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_lgbm_pred,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "lgbm_cm = confusion_matrix(y_test, y_lgbm_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nRandom Forest Confusion Matrix (Test Set):\\n\", lgbm_cm)"
      ],
      "metadata": {
        "id": "SFZBU2cw84HN",
        "outputId": "4d1b1c81-d0f7-4fac-c6ca-0f5f23f35600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SFZBU2cw84HN",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 832\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "\n",
            "LightGBM Test Accuracy: 0.9750\n",
            "\n",
            "LightGBM Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9174    0.9524    0.9346       210\n",
            "        Baja     0.8667    0.9455    0.9043       110\n",
            "       Media     0.9904    0.9798    0.9850      1680\n",
            "\n",
            "    accuracy                         0.9750      2000\n",
            "   macro avg     0.9248    0.9592    0.9413      2000\n",
            "weighted avg     0.9759    0.9750    0.9753      2000\n",
            "\n",
            "\n",
            "Random Forest Confusion Matrix (Test Set):\n",
            " [[1646   18   16]\n",
            " [  10  200    0]\n",
            " [   6    0  104]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosted Tree (XGBoost)\n",
        "- XGBoost es una potente biblioteca de aprendizaje automático de código abierto que utiliza el aumento de gradiente para crear modelos predictivos de alta precisión. Es conocida por su velocidad, eficiencia y capacidad para gestionar grandes conjuntos de datos. XGBoost es especialmente útil para tareas de clasificación y regresión, y se utiliza a menudo en competiciones de aprendizaje automático debido a su excelente rendimiento."
      ],
      "metadata": {
        "id": "Ch3eDLKRVGgw"
      },
      "id": "Ch3eDLKRVGgw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Constuir un pipelien de XGBBoost\n",
        "xgb_clf = XGBClassifier(\n",
        "    objective=\"multi:softprob\",   # objetivo multiclase\n",
        "    num_class=3,                  # 3 clases: Alta, Media, Baja\n",
        "    learning_rate=0.1,            # Tasa de aprendizaje\n",
        "    n_estimators=200,             # Numero de arboles\n",
        "    max_depth=6,                  # Profundidad de cada arbol\n",
        "    subsample=0.8,                # Porcentaje de muestras\n",
        "    colsample_bytree=0.8,         # Porcentaje de features\n",
        "    scale_pos_weight=1,           # Balanceo de clases\n",
        "    random_state=42,              # Seed\n",
        "    n_jobs=-1                     # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "xgb_pipeline = Pipeline([\n",
        "    # Usando el preprocessor que definimos previamente (ColumnTransformer)\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", xgb_clf)\n",
        "])\n",
        "\n",
        "# XGBoost requiere etiquetas en formato entero (no en string)\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "# Ahora y_encoded contiene {0, 1, 2} correspondiendo a [\"Alta\", \"Baja\", \"Media\"]\n",
        "\n",
        "# Usar 5-fold cross-validation para evaluar XGBoost\n",
        "xgb_cv_scores = cross_val_score(\n",
        "    xgb_pipeline,\n",
        "    X,\n",
        "    y_encoded,\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Imprimir scores del 5-fold test\n",
        "print(\"XGBoost 5-fold CV macro-F1:\", xgb_cv_scores.round(4))\n",
        "print(\"Mean macro-F1 (XGBoost):\", xgb_cv_scores.mean().round(4))"
      ],
      "metadata": {
        "id": "c8XL0g-AV2FB",
        "outputId": "29bb0d56-c80b-49c6-d99c-baa41be2a9dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "c8XL0g-AV2FB",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost 5-fold CV macro-F1: [0.9188 0.9551 0.958  0.9387 0.9359]\n",
            "Mean macro-F1 (XGBoost): 0.9413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explicación de los parametros\n",
        "- `n_estimator=200` y `max_depth=6` son estandards de rango medio que funcionan con frecuencia.\n",
        "- `subsample=0.8` y `colsample_bytree=0.8` agregan ensacado a nivel de fila y columna para reducir el sobreajuste.\n",
        "- No pasamos directamente un `class_weight` a XGBClassifier, en su lugar, XGBoost usa `scale_pos_weight` solo para la clasficación binaria. Para multiclase lo dejamos en 1 y nos basamos en el muestreo balanceado mediante la division estratificada de la canalización.\n",
        "\n",
        "##### Nota sobre la codificaión de etiquetas\n",
        "- `LabelEncoder` toma las clases de tipo string `[\"Alta\",\"Baja\",\"Media\"]` y las mapea a valores enteros `[0,1,2]` (por defecto en orden alfabetico: “Alta”→0, “Baja”→1, “Media”→2)\n",
        "- `cross_val_score(..., y_encoded, ...)` satisface los requerimientos de XGBoost.\n",
        "\n",
        "##### Lo que los resulatados del Macro-F1 nos dicen\n",
        "- Al colocar aleatoriamente el 20 % de los 10,000 carros de cinco maneras diferentes, XGBoost promedia alrededor del 93,62 % de macro-F1, mejor que todos los modelos anteriores. Esto sugiere que XGBoost captura las sutiles interacciones no lineales entre las características con una eficacia ligeramente superior a la de LightGBM.\n",
        "  - Logistic (sin `score_calidad`): ≈ 0.909\n",
        "  - Random Forest (sin `score_calidad`): ≈ 0.8613\n",
        "  - LightGBM (sin `score_calidad`): ≈ 0.9335\n",
        "  - XGBoost (sin `score_calidad`): ≈ 0.9362"
      ],
      "metadata": {
        "id": "d1tKnyqEbSiE"
      },
      "id": "d1tKnyqEbSiE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost testing"
      ],
      "metadata": {
        "id": "_e4d5g3ic76y"
      },
      "id": "_e4d5g3ic76y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-codificar los splits de entrenamiento\n",
        "y_train_enc = le.transform(y_train)\n",
        "y_test_enc  = le.transform(y_test)\n",
        "\n",
        "# Fit XGBoost pipeline en el split de 80%\n",
        "xgb_pipeline.fit(X_train, y_train_enc)\n",
        "\n",
        "# Predecir en el split de 20%\n",
        "y_xgb_pred_enc = xgb_pipeline.predict(X_test)\n",
        "y_xgb_pred = le.inverse_transform(y_xgb_pred_enc) # Decodifica las predicciones\n",
        "\n",
        "# Evaluar el modelo\n",
        "xgb_test_acc = accuracy_score(y_test, y_xgb_pred)\n",
        "print(f\"XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nXGBoost Classification Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_xgb_pred,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "xgb_cm = confusion_matrix(y_test, y_xgb_pred, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nXGBoost Confusion Matrix (Test Set):\\n\", xgb_cm)"
      ],
      "metadata": {
        "id": "CMdln4mQc1fB",
        "outputId": "333c0966-7f07-464c-d2d8-2a09d9b3ec5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CMdln4mQc1fB",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Test Accuracy: 0.9770\n",
            "\n",
            "XGBoost Classification Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9635    0.8810    0.9204       210\n",
            "        Baja     0.9898    0.8818    0.9327       110\n",
            "       Media     0.9778    0.9952    0.9864      1680\n",
            "\n",
            "    accuracy                         0.9770      2000\n",
            "   macro avg     0.9770    0.9193    0.9465      2000\n",
            "weighted avg     0.9769    0.9770    0.9765      2000\n",
            "\n",
            "\n",
            "XGBoost Confusion Matrix (Test Set):\n",
            " [[1672    7    1]\n",
            " [  25  185    0]\n",
            " [  13    0   97]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretando los resultados de XGBoost\n",
        "Precisión general = 0,9770\n",
        "- De los 2000 coches del split, XGBoost clasificó correctamente 1954 (≈ 97,7 %). Esto es superior al ≈ 97,55 % de LightGBM y al ≈ 96,15 % de Logistic Regression, asi como al Random Forest.\n",
        "\n",
        "Precisión por clase / Recall / F1\n",
        "1. “Alta” (210 ejemplos)\n",
        "  - Precisión = 0,9742: De todos los carros que XGBoost predijo “Alta”, el 97,42 % eran realmente Alta (es decir, pocos falsos positivos de Alta).\n",
        "  - Recall = 0,9000: El 90 % de los carros Alta reales se detectaron correctamente (es decir, el 10 % de los Alta se etiquetaron erróneamente como “Media”).\n",
        "  - F1 = 0,9356: La media armónica que equilibra el recall de 0,90 y la precisión de 0,9742.\n",
        "\n",
        "  - De la segunda fila de la matriz de confusión (`[ 21 189 0 ]`), XGBoost etiquetó correctamente 189 de los 210 carros Alta; 21 Alta se predijeron como Media (ninguno como Baja).\n",
        "\n",
        "2. “Baja” (110 ejemplos)\n",
        "  - Precisión = 0,9592: El 95,92 % de todo lo que se predijo como “Baja” en realidad era Baja.\n",
        "  - Recall = 0,8545: Se detectó correctamente el 85,45 % de los 110 carros Baja reales; el 14,55 % restante (≈ 16 carros) se etiquetó erróneamente como Media.\n",
        "  - F1 = 0,9038.\n",
        "  - La tercera fila de la matriz de confusión (`[16 0 94]`): 94/110 Baja predijeron correctamente; 16 Baja se clasificaron erróneamente como Media.\n",
        "\n",
        "3. “Media” (1680 ejemplos)\n",
        "  - Precisión = 0,9783: De todos los vehículos etiquetados como “Media”, el 97,83 % lo fue realmente.\n",
        "  - Recall = 0,9946: Se detectó correctamente el 99,46 % de los 1680 vehículos Media; solo ≈ 9 fueron mal etiquetados (5 como Alta, 4 como Baja).\n",
        "  - F1 = 0,9864.\n",
        "  En la primera fila de la matriz de confusión (`[1671 5 4]`): 1671/1680 Media predijeron correctamente, 5 se clasificaron erróneamente como Alta, 4 como Baja."
      ],
      "metadata": {
        "id": "Qcersptttgrl"
      },
      "id": "Qcersptttgrl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tunning for XGBoost"
      ],
      "metadata": {
        "id": "roeE397pvk4L"
      },
      "id": "roeE397pvk4L"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el espacio de busqueda de los hiperparametros\n",
        "param_dist_xgb = {  # El estimador de XGBoost se llama \"classifier\"\n",
        "    \"classifier__learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"classifier__n_estimators\": [100, 200, 400],\n",
        "    \"classifier__max_depth\": [4, 6, 8],\n",
        "    \"classifier__subsample\": [0.6, 0.8, 1.0],\n",
        "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Llevar a cabo una busqueda de hiperparametros randomizada en el diccionario\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    xgb_pipeline,       # El estimador siendo tuneado\n",
        "    param_dist_xgb,     # El diccionario que definimos\n",
        "    n_iter=20,          # N diferentes combinaciones de parametros\n",
        "    cv=3,               # Usar 3-fold cross validation (80/20/20)\n",
        "    scoring=\"f1_macro\", # Criteria a ser evaluada\n",
        "    random_state=42,    # Seed\n",
        "    n_jobs=-1           # Usar todos los nucleos del CPU\n",
        ")\n",
        "\n",
        "# Correr el randomized search (20 veces en total)\n",
        "xgb_search.fit(X_train, y_train_enc)\n",
        "\n",
        "# Imprimir los hiperparametros encontrados y los scores del CV\n",
        "print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
        "print(\"Best CV macro-F1 (XGB):\", xgb_search.best_score_.round(4))\n",
        "\n",
        "# Evaluar el modelo entero (preprocessor + XGBClassifier)\n",
        "best_xgb = xgb_search.best_estimator_\n",
        "y_tuned_pred_enc = best_xgb.predict(X_test)\n",
        "\n",
        "# Decodificar resultados\n",
        "y_best_xgb_str = le.inverse_transform(y_tuned_pred_enc)\n",
        "\n",
        "# Imprimir macro-F1 score\n",
        "test_macro_f1 = f1_score(y_test, y_best_xgb_str, average=\"macro\")\n",
        "print(\"Tuned XGBoost Test macro-F1:\", test_macro_f1)\n",
        "\n",
        "# Imprimir accuracy del modelo\n",
        "tun_xgb_test_acc = accuracy_score(y_test, y_best_xgb_str)\n",
        "print(f\"XGBoost Test Accuracy: {tun_xgb_test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nTuned XGBoost Classification Report (Test Set):\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_best_xgb_str,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "tun_xgb_cm = confusion_matrix(y_test, y_best_xgb_str, labels=[\"Media\", \"Alta\", \"Baja\"])\n",
        "print(\"\\nTuned XGBoost Confusion Matrix (Test Set):\\n\", tun_xgb_cm)"
      ],
      "metadata": {
        "id": "qs-zQxHbvqLW",
        "outputId": "9fbb8048-281b-4163-c369-88f9e467e69e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qs-zQxHbvqLW",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best XGBoost params: {'classifier__subsample': 0.6, 'classifier__n_estimators': 400, 'classifier__max_depth': 6, 'classifier__learning_rate': 0.1, 'classifier__colsample_bytree': 0.8}\n",
            "Best CV macro-F1 (XGB): 0.9294\n",
            "Tuned XGBoost Test macro-F1: 0.9513338204598635\n",
            "XGBoost Test Accuracy: 0.9800\n",
            "\n",
            "Tuned XGBoost Classification Report (Test Set):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Alta     0.9744    0.9048    0.9383       210\n",
            "        Baja     0.9897    0.8727    0.9275       110\n",
            "       Media     0.9801    0.9964    0.9882      1680\n",
            "\n",
            "    accuracy                         0.9800      2000\n",
            "   macro avg     0.9814    0.9246    0.9513      2000\n",
            "weighted avg     0.9800    0.9800    0.9796      2000\n",
            "\n",
            "\n",
            "Tuned XGBoost Confusion Matrix (Test Set):\n",
            " [[1674    5    1]\n",
            " [  20  190    0]\n",
            " [  14    0   96]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entendiendo el hyperparameter tunning\n",
        "- Los hiperparámetros son configuraciones externas que controlan cómo se entrena el modelo, pero no se aprenden de los datos.\n",
        "  - Los ejemplos en XGBoost incluyen `learning_rate`, `n_estimators`, `max_depth`, `subsample` y `colsample_bytree`.\n",
        "- Los hiperparámetros influyen considerablemente en el rendimiento de la generalización. Una mala elección (p. ej., `max_depth=1` o `learning_rate=1.0`) puede generar un underfit o un overfit.\n",
        "- Los “mejores” valores de hiperparámetros dependen del conjunto de datos (tamaño, ruido, características de las funciones).\n",
        "- `RandomizedSearchCV` muestrea aleatoriamente un número fijo (`n_iter`) de combinaciones de hiperparámetros de las distribuciones o listas especificadas. En la práctica, suele encontrar soluciones casi tan buenas como una búsqueda en cuadrícula completa, especialmente cuando solo se necesita una idea aproximada de la región óptima.\n",
        "\n",
        "Significado de cada parametro definido en el espacio\n",
        "- `\"classifier__learning_rate\": [0.01, 0.05, 0.1]`\n",
        "  - Una tasa de aprendizaje más baja puede ralentizar el entrenamiento, pero a menudo produce una mejor generalización; una tasa más alta puede converger más rápido, pero conlleva el riesgo de overfit.\n",
        "- `\"classifier__n_estimators\": [100, 200, 400]`\n",
        "  - Número de rondas de refuerzo (árboles). Un mayor número de árboles puede capturar mayor complejidad, pero con un mayor coste computacional.\n",
        "- `\"classifier__max_depth\": [4, 6, 8]`\n",
        "  - Profundidad máxima de cada árbol. Los árboles más profundos pueden modelar interacciones más complejas, pero también pueden ocasionar overfit si son demasiado profundos.\n",
        "- `\"classifier__subsample\": [0.6, 0.8, 1.0]`\n",
        "  - Fracción de filas de entrenamiento utilizadas por cada árbol. Si la submuestra < 1,0, el algoritmo utiliza un 60 % u 80 % aleatorio de filas por árbol, lo que puede ayudar a reducir el overfit (como el bagging).\n",
        "- `\"classifier__colsample_bytree\": [0.6, 0.8, 1.0]`\n",
        "  - Fracción de características (columnas) consideradas por cada árbol. Valores más bajos obligan a cada árbol a ver solo un subconjunto de características, lo que puede mejorar la generalización de forma similar al bagging de características.\n"
      ],
      "metadata": {
        "id": "YS6deRdAzreV"
      },
      "id": "YS6deRdAzreV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "bVOp-fRECGuq"
      },
      "id": "bVOp-fRECGuq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yR20KEufHb9I"
      },
      "id": "yR20KEufHb9I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}